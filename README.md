# ğŸ¤Ÿ Real-Time Sign Language Recognition System

This project uses **deep learning** and **computer vision** to recognize American Sign Language (ASL) alphabets in real time through a webcam. It combines a **CNN model trained on ASL images** with **MediaPipe hand tracking** and **OpenCV visualization** to create an interactive and accurate gesture recognition system.

---
##  Research paper Link

- [American Sign Language Alphabet Recognition using CNN and MediaPipe â€“ IEEE Xplore](https://ieeexplore.ieee.org/document/10968373)

## ğŸš€ Features

- Real-time hand gesture detection using webcam
- ASL alphabet classification using trained CNN model
- Top-3 predictions with confidence scores on screen
- Hand landmark visualization using MediaPipe
- Dataset augmentation for improved model robustness

---
## ğŸ§° Technologies Used

### ğŸ§  Machine Learning / Deep Learning
- **TensorFlow** + **Keras** â€“ Designing and training the CNN model
- **ImageDataGenerator** â€“ For on-the-fly image augmentation during training
- **NumPy** â€“ Efficient numerical operations and image array manipulation

### ğŸ“¸ Computer Vision
- **OpenCV** â€“ Capturing webcam feed and rendering real-time hand visualizations
- **MediaPipe** â€“ Real-time hand landmark detection from video frames

### ğŸ“Š Evaluation & Visualization
- **scikit-learn** â€“ Evaluation metrics like confusion matrix, classification report, ROC AUC
- **matplotlib** â€“ Plotting confusion matrix, ROC curve, and saving sample predictions

## âš™ï¸ Installation & Usage Guide

Follow these steps to set up the project locally using a Python virtual environment:

---
## ğŸ“ Dataset

This project uses a preprocessed American Sign Language (ASL) dataset based on MediaPipe hand landmarks.

You can download the dataset from Kaggle:

ğŸ”— [MediaPipe Processed ASL Dataset (by vignonantoine)](https://www.kaggle.com/datasets/vignonantoine/mediapipe-processed-asl-dataset)

### Step 1: Clone the Repository

```bash
git clone https://github.com/arindampattanayak/american-sign-language-recognition.git
cd american-sign-language-recognition
```
### Step 2: Create a Virtual Environment

ğŸªŸ For Windows:
   ```sh
python -m venv venv
venv\Scripts\activate
```

ğŸ§ For macOS/Linux:
   ```sh
python3 -m venv venv
source venv/bin/activate
```
 ### Step 3: Install Dependencies
 
 Make sure you have requirements.txt in your project folder. Then run:
 ```sh
 pip install -r requirements.txt
```
 If you donâ€™t have one yet, manually install the core packages:
```sh
 pip install tensorflow opencv-python mediapipe scikit-learn matplotlib numpy
```
 ### Step 4: Update File Paths
Before running the scripts below, edit the paths in the following Python files:
Example paths to update
 ```sh
model_path = "path/to/your/model"
data_path = "path/to/your/dataset"
result_path = "path/to/save/results"
```
### Step 5: Train the Model

 ```sh
python train.py
```
This script will:
- Load the dataset
- Train a CNN model on ASL images
- Save the model with your chosen filename in your specified directory

### Step 6: Run Real-Time Prediction (Test Webcam)
Make sure your webcam is connected:
```sh
python test.py
```
This script will:
 - Start webcam feed
 - Detect and track your hand using MediaPipe
 - Predict the sign in real-time and display top-3 predictions with confidence

 ### Step 7: Evaluate Model Performance

To evaluate the model on a subset of your dataset:
 ```sh
 python result.py
```
This script will:
 - Generate a classification report
 - Save a confusion matrix image
 - Save a ROC AUC curve image
 - Save sample prediction visualizations and a summary report

## ğŸ™Œ Acknowledgements

- [MediaPipe](https://mediapipe.dev/)
- [Dataset by vignonantoine](https://www.kaggle.com/datasets/vignonantoine/mediapipe-processed-asl-dataset)






